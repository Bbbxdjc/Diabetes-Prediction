{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReliefF calculated importance order: ['Glucose', 'BMI', 'SkinThickness', 'Pregnancies', 'Age', 'DiabetesPedigreeFunction', 'BloodPressure', 'Insulin']\n",
      "Auto generated region mapping:\n",
      "Glucose: (0, 0, 80, 80)\n",
      "BMI: (80, 0, 40, 60)\n",
      "SkinThickness: (0, 80, 50, 40)\n",
      "Pregnancies: (50, 80, 40, 30)\n",
      "Age: (80, 60, 40, 20)\n",
      "DiabetesPedigreeFunction: (90, 80, 30, 20)\n",
      "BloodPressure: (50, 110, 25, 10)\n",
      "Insulin: (75, 110, 20, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Round 1 Training (each round: 10 epochs) -----\n",
      "Epoch 1/10 - Train Loss: 0.5524 - Train Acc: 70.36% - Test Loss: 0.6649 - Test Acc: 63.64%\n",
      "Epoch 2/10 - Train Loss: 0.4813 - Train Acc: 77.52% - Test Loss: 0.4858 - Test Acc: 79.22%\n",
      "Epoch 3/10 - Train Loss: 0.4565 - Train Acc: 76.87% - Test Loss: 0.5532 - Test Acc: 76.62%\n",
      "Epoch 4/10 - Train Loss: 0.4372 - Train Acc: 78.50% - Test Loss: 0.5245 - Test Acc: 74.68%\n",
      "Epoch 5/10 - Train Loss: 0.4386 - Train Acc: 78.34% - Test Loss: 0.5005 - Test Acc: 74.03%\n",
      "Epoch 6/10 - Train Loss: 0.4312 - Train Acc: 80.13% - Test Loss: 0.5183 - Test Acc: 74.68%\n",
      "Epoch 7/10 - Train Loss: 0.3828 - Train Acc: 81.11% - Test Loss: 0.4951 - Test Acc: 76.62%\n",
      "Epoch 8/10 - Train Loss: 0.3772 - Train Acc: 82.25% - Test Loss: 0.5345 - Test Acc: 77.92%\n",
      "Epoch 9/10 - Train Loss: 0.3631 - Train Acc: 83.55% - Test Loss: 0.5632 - Test Acc: 74.68%\n",
      "Epoch 10/10 - Train Loss: 0.3653 - Train Acc: 83.55% - Test Loss: 0.5313 - Test Acc: 75.32%\n",
      "Round 1 -> Test Accuracy: 75.32%\n",
      "Round 1: Found 96 misclassified original samples.\n",
      "After Round 1, new training dataset size: 710\n",
      "\n",
      "----- Round 2 Training (each round: 10 epochs) -----\n",
      "Epoch 1/10 - Train Loss: 0.4461 - Train Acc: 77.75% - Test Loss: 0.5835 - Test Acc: 68.83%\n",
      "Epoch 2/10 - Train Loss: 0.3689 - Train Acc: 82.96% - Test Loss: 0.6038 - Test Acc: 70.78%\n",
      "Epoch 3/10 - Train Loss: 0.3418 - Train Acc: 85.07% - Test Loss: 0.6277 - Test Acc: 68.18%\n",
      "Epoch 4/10 - Train Loss: 0.2874 - Train Acc: 89.01% - Test Loss: 0.5631 - Test Acc: 74.68%\n",
      "Epoch 5/10 - Train Loss: 0.2976 - Train Acc: 87.46% - Test Loss: 0.6077 - Test Acc: 73.38%\n",
      "Epoch 6/10 - Train Loss: 0.2422 - Train Acc: 91.41% - Test Loss: 0.6624 - Test Acc: 74.68%\n",
      "Epoch 7/10 - Train Loss: 0.2193 - Train Acc: 91.13% - Test Loss: 0.7201 - Test Acc: 71.43%\n",
      "Epoch 8/10 - Train Loss: 0.2057 - Train Acc: 92.11% - Test Loss: 0.6869 - Test Acc: 72.73%\n",
      "Epoch 9/10 - Train Loss: 0.1991 - Train Acc: 91.69% - Test Loss: 0.6871 - Test Acc: 71.43%\n",
      "Epoch 10/10 - Train Loss: 0.1656 - Train Acc: 93.38% - Test Loss: 0.6907 - Test Acc: 72.73%\n",
      "Round 2 -> Test Accuracy: 72.73%\n",
      "Test accuracy improvement is minimal, stopping further rounds.\n",
      "Final Test Accuracy: 72.73%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# ---------------------- Set random seed ----------------------\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---------------------- 1. Data load and normalization ----------------------\n",
    "feature_cols = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\",\n",
    "                \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "\n",
    "data = pd.read_csv('diabetes_dataset_1.csv')\n",
    "data_norm = data.copy()\n",
    "for col in feature_cols:\n",
    "    min_val = data[col].min()\n",
    "    max_val = data[col].max()\n",
    "    data_norm[col] = (data[col] - min_val) / (max_val - min_val)\n",
    "\n",
    "# ---------------------- 2. Use ReliefF to calculate feature importance ----------------------\n",
    "from skrebate import ReliefF\n",
    "\n",
    "X = data_norm[feature_cols].values\n",
    "y = data_norm['Outcome'].values\n",
    "relief = ReliefF(n_neighbors=10)\n",
    "relief.fit(X, y)\n",
    "importance_scores = relief.feature_importances_\n",
    "ordered_indices = np.argsort(importance_scores)[::-1]\n",
    "ordered_features = [feature_cols[i] for i in ordered_indices]\n",
    "print(\"ReliefF calculated importance order:\", ordered_features)\n",
    "\n",
    "# ---------------------- 3. Generate region mappings ----------------------\n",
    "predefined_regions = [\n",
    "    (0, 0, 80, 80),    # Area: 6400, for most important feature\n",
    "    (80, 0, 40, 60),   # Area: 2400\n",
    "    (0, 80, 50, 40),   # Area: 2000\n",
    "    (50, 80, 40, 30),  # Area: 1200\n",
    "    (80, 60, 40, 20),  # Area: 800\n",
    "    (90, 80, 30, 20),  # Area: 600\n",
    "    (50, 110, 25, 10), # Area: 250\n",
    "    (75, 110, 20, 10)  # Area: 200\n",
    "]\n",
    "auto_region_mapping = {}\n",
    "for i, feat in enumerate(ordered_features):\n",
    "    auto_region_mapping[feat] = predefined_regions[i]\n",
    "\n",
    "print(\"Auto generated region mapping:\")\n",
    "for feat, region in auto_region_mapping.items():\n",
    "    print(f\"{feat}: {region}\")\n",
    "\n",
    "def create_image_from_features(sample, region_mapping):\n",
    "    \"\"\"\n",
    "    Generate a 120x120 grayscale image from a sample.\n",
    "    \"\"\"\n",
    "    img = Image.new('L', (120, 120), color=0)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for feat in feature_cols:\n",
    "        val = int(sample[feat] * 255)\n",
    "        if feat in region_mapping:\n",
    "            x, y, w, h = region_mapping[feat]\n",
    "            draw.rectangle([x, y, x + w, y + h], fill=val)\n",
    "    return img\n",
    "\n",
    "# 可视化一个样本转换后的图像\n",
    "sample_img = create_image_from_features(data_norm.iloc[0], auto_region_mapping)\n",
    "plt.imshow(sample_img, cmap='gray')\n",
    "plt.title(\"Sample Converted Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# ---------------------- 4. Define PimaImageDataset with orig_index ----------------------\n",
    "class PimaImageDataset(Dataset):\n",
    "    def __init__(self, X, y, orig_indices=None, transform=None):\n",
    "        \"\"\"\n",
    "        X: list of images (PIL.Image or tensor)\n",
    "        y: list of labels\n",
    "        orig_indices: list of original indices; for original samples, a valid index (0,1,2,...),\n",
    "                      for augmented samples, use None.\n",
    "        transform: transformation to apply (e.g., ToTensor)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        if orig_indices is None:\n",
    "            self.orig_indices = list(range(len(X)))\n",
    "        else:\n",
    "            self.orig_indices = orig_indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx]\n",
    "        label = int(self.y[idx])\n",
    "        orig_index = self.orig_indices[idx]\n",
    "        # 如果 orig_index 为 None，则返回 -1 作为占位符\n",
    "        if orig_index is None:\n",
    "            orig_index = -1\n",
    "        if torch.is_tensor(img):\n",
    "            return img, label, orig_index\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = transforms.ToTensor()(img)\n",
    "        return img, label, orig_index\n",
    "\n",
    "# ---------------------- 5. Convert table data to images ----------------------\n",
    "def convert_to_rgb(img):\n",
    "    return img.convert('RGB')\n",
    "\n",
    "def changeToTwoD(dataset, region_mapping):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for idx in range(dataset.shape[0]):\n",
    "        img = convert_to_rgb(create_image_from_features(dataset.iloc[idx], region_mapping))\n",
    "        resize = transforms.Resize(size=(224, 224))\n",
    "        img = resize(img)\n",
    "        label = int(dataset.iloc[idx]['Outcome'])\n",
    "        features.append(img)\n",
    "        labels.append(label)\n",
    "    return features, labels\n",
    "\n",
    "# 使用 changeToTwoD 仅生成原始图像，不做几何增广\n",
    "img_features, img_labels = changeToTwoD(data_norm, auto_region_mapping)\n",
    "# 为原始样本设置 orig_indices 为 0,1,...,len(img_features)-1\n",
    "orig_indices = list(range(len(img_features)))\n",
    "\n",
    "# ---------------------- 6. Split data and build DataLoader ----------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, orig_train_indices, orig_test_indices = train_test_split(\n",
    "    img_features, img_labels, orig_indices, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 定义转换，将 PIL.Image 转为 tensor\n",
    "to_tensor = transforms.ToTensor()\n",
    "train_dataset = PimaImageDataset(X_train, y_train, orig_indices=orig_train_indices, transform=to_tensor)\n",
    "test_dataset = PimaImageDataset(X_test, y_test, orig_indices=orig_test_indices, transform=to_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "# ---------------------- 7. Define and finetune the pretrained ResNet18 Model ----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "num_features_model = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_features_model, 2)  # 2-class classification\n",
    "model = resnet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "epochs_per_round = 10\n",
    "max_num_rounds = 5  # 最大训练轮数（外层迭代）\n",
    "overall_round = 0\n",
    "max_duplications = 2\n",
    "pre_test_acc = 0\n",
    "\n",
    "# duplication_counts: 针对原始样本（orig_index 不为 None）的复制计数\n",
    "duplication_counts = {idx: 0 for idx in orig_train_indices}\n",
    "\n",
    "while overall_round < max_num_rounds:\n",
    "    overall_round += 1\n",
    "    print(f\"\\n----- Round {overall_round} Training (each round: {epochs_per_round} epochs) -----\")\n",
    "    \n",
    "    # 训练 epochs_per_round 个 epoch\n",
    "    for epoch in range(epochs_per_round):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for imgs, labels, _ in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (preds == labels).sum().item()\n",
    "        epoch_loss = running_loss / total_train\n",
    "        train_acc = correct_train / total_train * 100\n",
    "   \n",
    "        # 测试集评估\n",
    "        model.eval()\n",
    "        running_test_loss = 0.0\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels, _ in test_loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_test_loss += loss.item() * imgs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (preds == labels).sum().item()\n",
    "        test_loss = running_test_loss / total_test\n",
    "        test_acc = correct_test / total_test * 100\n",
    "        print(f\"Epoch {epoch+1}/{epochs_per_round} - Train Loss: {epoch_loss:.4f} - Train Acc: {train_acc:.2f}% - Test Loss: {test_loss:.4f} - Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels, _ in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    final_test_acc = correct / total * 100\n",
    "    print(f\"Round {overall_round} -> Test Accuracy: {final_test_acc:.2f}%\")\n",
    "    \n",
    "    \n",
    "    # 判断改进是否足够显著\n",
    "    if final_test_acc - pre_test_acc < 1e-4:\n",
    "        print(\"Test accuracy improvement is minimal, stopping further rounds.\")\n",
    "        break\n",
    "    pre_test_acc = test_acc\n",
    "    \n",
    "    # ---------------------- Extract misclassified original samples from current training dataset ----------------------\n",
    "    misclassified_imgs = []\n",
    "    misclassified_labels = []\n",
    "    # 遍历整个训练集，不使用 DataLoader（保证顺序一致）\n",
    "    for i in range(len(train_dataset)):\n",
    "        img, label, orig_idx = train_dataset[i]\n",
    "        # 仅对原始样本（orig_idx 不为 None）进行判断\n",
    "        if orig_idx != -1:\n",
    "            # 将单个样本添加 batch 维度\n",
    "            img_input = img.unsqueeze(0).to(device)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model(img_input)\n",
    "                _, pred = torch.max(output, 1)\n",
    "            if pred.item() != label:\n",
    "                # 如果复制次数未达到上限，则复制该样本\n",
    "                if duplication_counts[orig_idx] < max_duplications:\n",
    "                    duplication_counts[orig_idx] += 1\n",
    "                    misclassified_imgs.append(img)  # 原样复制\n",
    "                    misclassified_labels.append(label)\n",
    "    print(f\"Round {overall_round}: Found {len(misclassified_imgs)} misclassified original samples.\")\n",
    "    if len(misclassified_imgs) == 0:\n",
    "        print(\"No misclassified original samples found, stopping further rounds.\")\n",
    "        break\n",
    "\n",
    "    # ---------------------- Construct new training dataset ----------------------\n",
    "    # 仅取原始训练数据（即 train_dataset 中 orig_idx 不为 None）的样本\n",
    "    original_train_imgs = []\n",
    "    original_train_labels = []\n",
    "    original_train_orig = []\n",
    "    for img, label, orig_idx in train_dataset:\n",
    "        if orig_idx is not None:\n",
    "            original_train_imgs.append(img)\n",
    "            original_train_labels.append(label)\n",
    "            original_train_orig.append(orig_idx)\n",
    "    # 新训练集 = 原始训练样本 + 新复制的误分类样本（复制样本的 orig_index 设为 None）\n",
    "    new_train_imgs = original_train_imgs + misclassified_imgs\n",
    "    new_train_labels = original_train_labels + misclassified_labels\n",
    "    new_train_orig = original_train_orig + [None] * len(misclassified_imgs)\n",
    "    \n",
    "    # 构造新的数据集和 DataLoader\n",
    "    train_dataset = PimaImageDataset(new_train_imgs, new_train_labels, orig_indices=new_train_orig, transform=to_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    print(f\"After Round {overall_round}, new training dataset size: {len(train_dataset)}\")\n",
    "\n",
    "# ---------------------- 10. Final evaluation on test set ----------------------\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels, _ in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "final_test_acc = correct / total * 100\n",
    "print(f\"Final Test Accuracy: {final_test_acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
