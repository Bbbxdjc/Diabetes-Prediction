{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReliefF 计算的特征重要性排序： ['Glucose', 'BMI', 'SkinThickness', 'Pregnancies', 'Age', 'DiabetesPedigreeFunction', 'BloodPressure', 'Insulin']\n",
      "自动生成的区域映射：\n",
      "Glucose: (0, 0, 80, 80)\n",
      "BMI: (80, 0, 40, 60)\n",
      "SkinThickness: (0, 80, 50, 40)\n",
      "Pregnancies: (50, 80, 40, 30)\n",
      "Age: (80, 60, 40, 20)\n",
      "DiabetesPedigreeFunction: (90, 80, 30, 20)\n",
      "BloodPressure: (50, 110, 25, 10)\n",
      "Insulin: (75, 110, 20, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 初始训练 -----\n",
      "Epoch 1/10 - Loss: 0.5270 - Train Acc: 73.86% - Test Loss: 0.6028 - Test Acc: 69.27%\n",
      "Epoch 2/10 - Loss: 0.4902 - Train Acc: 75.23% - Test Loss: 0.5247 - Test Acc: 73.31%\n",
      "Epoch 3/10 - Loss: 0.4421 - Train Acc: 78.45% - Test Loss: 0.5118 - Test Acc: 73.05%\n",
      "Epoch 4/10 - Loss: 0.4104 - Train Acc: 80.60% - Test Loss: 0.5445 - Test Acc: 73.70%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 209\u001b[0m\n\u001b[0;32m    207\u001b[0m imgs, labels \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    208\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 209\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m    211\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torchvision\\models\\resnet.py:273\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    270\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[1;32m--> 273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[1;32md:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torchvision\\models\\resnet.py:96\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     93\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m     94\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m---> 96\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\AppsFiles\\Anaconda3\\envs\\tor_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "        \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "# ---------------------- 1. 数据加载与归一化 ----------------------\n",
    "# CSV 文件包含的特征\n",
    "feature_cols = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\",\n",
    "                \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "\n",
    "data = pd.read_csv('diabetes_dataset_1.csv')\n",
    "\n",
    "# 对所有特征进行 min–max 归一化\n",
    "data_norm = data.copy()\n",
    "for col in feature_cols:\n",
    "    min_val = data[col].min()\n",
    "    max_val = data[col].max()\n",
    "    data_norm[col] = (data[col] - min_val) / (max_val - min_val)\n",
    "\n",
    "# ---------------------- 2. 利用 ReliefF 自动计算特征重要性 ----------------------\n",
    "# 需要安装 skrebate：pip install skrebate\n",
    "from skrebate import ReliefF\n",
    "\n",
    "X = data_norm[feature_cols].values\n",
    "y = data_norm['Outcome'].values\n",
    "\n",
    "# 初始化 ReliefF 算法（邻居数设为10）\n",
    "relief = ReliefF(n_neighbors=10)\n",
    "relief.fit(X, y)\n",
    "importance_scores = relief.feature_importances_\n",
    "\n",
    "# 按照重要性降序排列，得到特征名称排序\n",
    "ordered_indices = np.argsort(importance_scores)[::-1]\n",
    "ordered_features = [feature_cols[i] for i in ordered_indices]\n",
    "print(\"ReliefF 计算的特征重要性排序：\", ordered_features)\n",
    "\n",
    "# ---------------------- 3. 根据排序自动生成区域映射 ----------------------\n",
    "# 定义预先设计好的区域列表，按面积从大到小排列（格式：(x, y, width, height)）\n",
    "predefined_regions = [\n",
    "    (0, 0, 80, 80),    # 面积：80x80=6400，分配给最重要的特征\n",
    "    (80, 0, 40, 60),   # 面积：40x60=2400\n",
    "    (0, 80, 50, 40),   # 面积：50x40=2000\n",
    "    (50, 80, 40, 30),  # 面积：40x30=1200\n",
    "    (80, 60, 40, 20),  # 面积：40x20=800\n",
    "    (90, 80, 30, 20),  # 面积：30x20=600\n",
    "    (50, 110, 25, 10), # 面积：25x10=250\n",
    "    (75, 110, 20, 10)  # 面积：20x10=200\n",
    "]\n",
    "\n",
    "self_region_mapping = {\n",
    "    \"Age\": (0, 0, 69, 60),                     # Feature_7\n",
    "    \"SkinThickness\": (69, 0, 51, 60),          # Feature_3\n",
    "    \"BMI\": (0, 60, 59, 40),                    # Feature_5\n",
    "    \"Pregnancies\": (59, 60, 32, 40),           # Feature_0\n",
    "    \"Insulin\": (91, 60, 29, 40),               # Feature_4\n",
    "    \"DiabetesPedigreeFunction\": (0, 100, 75, 20),  # Feature_6\n",
    "    \"BloodPressure\": (75, 100, 39, 20),        # Feature_2\n",
    "    \"Glucose\": (114, 100, 6, 20)               # Feature_1\n",
    "}\n",
    "\n",
    "# 自动将预定义区域分配给排序后的特征\n",
    "auto_region_mapping = {}\n",
    "for i, feat in enumerate(ordered_features):\n",
    "    auto_region_mapping[feat] = predefined_regions[i]\n",
    "\n",
    "print(\"自动生成的区域映射：\")\n",
    "for feat, region in auto_region_mapping.items():\n",
    "    print(f\"{feat}: {region}\")\n",
    "\n",
    "\n",
    "def create_image_from_features(sample, region_mapping):\n",
    "    \"\"\"\n",
    "    根据单个样本的归一化特征和给定的 region_mapping 生成 120x120 灰度图像\n",
    "    \"\"\"\n",
    "    img = Image.new('L', (120, 120), color=0)  # 创建黑底图\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # 对于每个特征，根据 mapping 中的区域将值映射到 0-255 灰度值\n",
    "    for feat in feature_cols:\n",
    "        val = int(sample[feat] * 255)\n",
    "        if feat in region_mapping:\n",
    "            x, y, w, h = region_mapping[feat]\n",
    "            draw.rectangle([x, y, x + w, y + h], fill=val)\n",
    "    return img\n",
    "\n",
    "# 可视化查看一个样本转换后的图像\n",
    "sample_img = create_image_from_features(data_norm.iloc[0], auto_region_mapping)\n",
    "plt.imshow(sample_img, cmap='gray')\n",
    "plt.title(\"Sample Converted Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------- 5. 定义 PyTorch 数据集 ----------------------\n",
    "class PimaImageDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        \"\"\"\n",
    "        df：包含归一化特征及 Outcome 的 DataFrame\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx]\n",
    "        label = int(self.y[idx])\n",
    "        # 如果 transform 已经设置，则直接应用\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        # 否则，如果 img 不是 tensor，则转换为 tensor；如果已经是 tensor，则直接返回\n",
    "        elif not torch.is_tensor(img):\n",
    "            img = transforms.ToTensor()(img)\n",
    "        return img, label\n",
    "\n",
    "# ---------------------- 6. 数据增强与数据加载器 ----------------------\n",
    "def convert_to_rgb(img):\n",
    "    return img.convert('RGB')\n",
    "\n",
    "# 一维转二维\n",
    "def changeToTwoD(dataset, region_mapping):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for idx in range(dataset.shape[0]):\n",
    "        img = convert_to_rgb(create_image_from_features(dataset.iloc[idx], region_mapping))\n",
    "\n",
    "        resize = transforms.Resize(size=(224, 224))\n",
    "        img = resize.forward(img)\n",
    "\n",
    "        label = int(dataset.iloc[idx]['Outcome'])\n",
    "        features.append(img)\n",
    "        labels.append(label)\n",
    "    return features, labels\n",
    "\n",
    "# 这里对2D图像通过几何变换进行数据增广\n",
    "def create_artificial_records(img_features, img_labels):\n",
    "    artificial_features = []\n",
    "    artificial_labels = []\n",
    "    for i, imfe in enumerate(img_features):\n",
    "\n",
    "        img = [0] * 4\n",
    "\n",
    "        flip = transforms.RandomHorizontalFlip(p=1)\n",
    "        rotate = transforms.RandomRotation(30)\n",
    "        scale = transforms.Resize(size=(int(120 * random.uniform(0.9,1.1)), int(120 * random.uniform(0.9,1.1))))\n",
    "        translate = transforms.RandomAffine(0, translate=(0.1, 0.1))\n",
    "\n",
    "        resize = transforms.Resize(size=(224, 224))\n",
    "\n",
    "        img[0] = flip.forward(imfe)\n",
    "        img[1] = rotate.forward(imfe)\n",
    "        img[2] = scale.forward(imfe)\n",
    "        img[3] = translate.forward(imfe)\n",
    "        img = [resize.forward(_im) for _im in img]\n",
    "        artificial_features += img\n",
    "        artificial_labels += [img_labels[i]] * 4\n",
    "    return artificial_features, artificial_labels\n",
    "\n",
    "img_features, img_labels = changeToTwoD(data_norm, auto_region_mapping)\n",
    "aug_img_features, aug_img_labels = create_artificial_records(img_features, img_labels)\n",
    "all_features = img_features + aug_img_features\n",
    "all_labels = img_labels + aug_img_labels\n",
    "\n",
    "# 下面还要改改\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_features, all_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = PimaImageDataset(X_train, y_train)\n",
    "test_dataset = PimaImageDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "# ---------------------- 7. 定义并微调预训练的 ResNet18 模型 ----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # 两分类任务\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 10\n",
    "print(\"----- 初始训练 -----\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "    epoch_loss = running_loss / total_train\n",
    "    train_acc = correct_train / total_train * 100\n",
    "    \n",
    "    \n",
    "     # 测试集评估\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_test_loss += loss.item() * imgs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (preds == labels).sum().item()\n",
    "    test_loss = running_test_loss / total_test\n",
    "    test_acc = correct_test / total_test * 100\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Train Acc: {train_acc:.2f}% - Test Loss: {test_loss:.4f} - Test Acc: {test_acc:.2f}%\")\n",
    "    # print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Train Acc: {train_acc:.2f}% - Test Acc: {test_acc:.2f}%\")\n",
    "    \n",
    "\n",
    "##############################################\n",
    "# 5. 从初始训练中提取误分类图片并复制（仅复制原样，不做内部微调）\n",
    "##############################################\n",
    "model.eval()\n",
    "misclassified_imgs = []\n",
    "misclassified_labels = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for i in range(len(labels)):\n",
    "            if preds[i] != labels[i]:\n",
    "                # 将误分类图片直接复制（原样，不进行变换）\n",
    "                misclassified_imgs.append(imgs[i].cpu())\n",
    "                misclassified_labels.append(labels[i].cpu())\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "final_test_acc = correct / total * 100\n",
    "print(f\"Final Test Accuracy: {final_test_acc:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"在训练集中发现误分类图片数：{len(misclassified_imgs)}\")\n",
    "\n",
    "# 此处仅复制原样，不进行几何变换\n",
    "additional_imgs = misclassified_imgs.copy()\n",
    "additional_labels = [int(lbl) for lbl in misclassified_labels]\n",
    "\n",
    "print(f\"增强生成的额外样本数：{len(additional_imgs)}\")\n",
    "\n",
    "# 从原始训练数据集中提取所有图像（以 tensor 形式）\n",
    "original_train_imgs = []\n",
    "original_train_labels = []\n",
    "for img, label in train_loader.dataset:\n",
    "    original_train_imgs.append(img)\n",
    "    original_train_labels.append(label)\n",
    "\n",
    "# 合并原始训练集与复制的误分类图片\n",
    "new_train_imgs = original_train_imgs + additional_imgs\n",
    "new_train_labels = original_train_labels + additional_labels\n",
    "\n",
    "# 构造新的训练集\n",
    "new_train_dataset = PimaImageDataset(new_train_imgs, new_train_labels)\n",
    "new_train_loader = DataLoader(new_train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "##############################################\n",
    "# 6. 使用包含误分类复制样本的新训练集重新训练模型\n",
    "##############################################\n",
    "num_additional_epochs = 10\n",
    "print(\"----- 重新训练：使用增强后的训练集 -----\")\n",
    "for epoch in range(num_additional_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for imgs, labels in new_train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total * 100\n",
    "    \n",
    "    \n",
    "     # 测试集评估\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_test_loss += loss.item() * imgs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (preds == labels).sum().item()\n",
    "    test_loss = running_test_loss / total_test\n",
    "    test_acc = correct_test / total_test * 100\n",
    "    \n",
    "    print(f\"Additional Epoch {epoch+1}/{num_additional_epochs} - Loss: {epoch_loss:.4f} - Train Acc: {train_acc:.2f}% - Test Loss: {test_loss:.4f} - Test Acc: {test_acc:.2f}%\")\n",
    "    \n",
    "    # print(f\"Additional Epoch {epoch+1}/{num_additional_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "##############################################\n",
    "# 7. 最终评估模型在测试集上的性能\n",
    "##############################################\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "final_test_acc = correct / total * 100\n",
    "print(f\"Final Test Accuracy: {final_test_acc:.2f}%\")\n",
    "\n",
    "\n",
    "# ---------------------- 10. 绘制指标曲线 ----------------------\n",
    "epochs = range(1, num_epochs + 1)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 绘制 Loss 曲线\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss_list, 'b-', label='Train Loss')\n",
    "plt.plot(epochs, test_loss_list, 'r-', label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Test Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# 绘制 Accuracy 曲线\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_acc_list, 'b-', label='Train Accuracy')\n",
    "plt.plot(epochs, test_acc_list, 'r-', label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Train and Test Accuracy over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------- 11. 可视化部分 ----------------------\n",
    "model.eval()\n",
    "data_iter = iter(test_loader)\n",
    "images, labels = next(data_iter)\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "outputs = model(images)\n",
    "_, preds = torch.max(outputs, 1)\n",
    "\n",
    "images_np = images.cpu().numpy().transpose((0, 2, 3, 1))\n",
    "\n",
    "batch_size = images_np.shape[0]\n",
    "cols = 8\n",
    "rows = batch_size // cols if batch_size % cols == 0 else batch_size // cols + 1\n",
    "plt.figure(figsize=(15, 2.5 * rows))\n",
    "for i in range(batch_size):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(images_np[i])\n",
    "    plt.title(f\"Pred: {preds[i].item()}\\nTrue: {labels[i].item()}\", fontsize=8)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
